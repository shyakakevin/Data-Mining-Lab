<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Book Summary â€“ Introduction to Statistical Learning</title>
  <style>
    /* ----------  BASIC LAYOUT  ---------- */
    body{
      font-family:"Segoe UI",Arial,sans-serif;
      background:#f4f6f8;
      margin:0;
      padding:48px 32px;
      line-height:1.65;
      color:#263238;
    }
    h1,h2,h3{color:#0d47a1;margin-top:0.2em}
    a{color:#1565c0;text-decoration:none}
    a:hover{text-decoration:underline}
    /* ----------  CONTAINER / CARD  ---------- */
    .card{
      background:#fff;
      border-radius:12px;
      padding:28px;
      margin-bottom:40px;
      box-shadow:0 3px 10px rgba(0,0,0,.08);
    }
    .chapter{border-left:6px solid #0d47a1;padding-left:18px}
    .toc{background:#e3f2fd}
    ul,ol{padding-left:24px}
    footer{text-align:center;margin-top:60px;font-style:italic}
  </style>
</head>
<body>

<!-- â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   HEADER   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ -->
<div class="card">
  <h1>ğŸ“˜ An Introduction to Statistical Learning with Applications in Python</h1>
  <p><strong>Name:</strong> ShyakaÂ Kevin</p>
  <p><strong>StudentÂ ID:</strong> 100915</p>
  <p><strong>GitHub Labs (1&nbsp;â€“&nbsp;8):</strong>
     <a href="https://github.com/shyakakevin/Data-Mining-Lab" target="_blank">
       https://github.com/shyakakevin/Data-Mining-Lab
     </a>
  </p>
</div>

<!-- â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   OVERVIEW   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ -->
<div class="card">
  <h2>ğŸ“š Overview</h2>
  <p>
    <em>An Introduction to Statistical Learning</em> (ISLP) translates cuttingâ€‘edge statistical
    and machineâ€‘learning methods into Pythonâ€‘based practice.  Covering regression,
    classification, resampling, regularization, splines, tree ensembles, SVMs, and deep
    networks, it balances mathematical intuition with handsâ€‘on labs that use real data
    such as <strong>Advertising</strong>, <strong>Boston Housing</strong>, <strong>Smarket</strong>, and <strong>Carseats</strong>.
    The text highlights themes like the <strong>biasâ€‘variance tradeâ€‘off</strong>,
    <strong>model interpretability</strong>, and <strong>crossâ€‘validation</strong>, positioning the reader to build
    robust predictive models and understand their limitations.
  </p>
</div>

<!-- â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   TABLE OF CONTENTS   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ -->
<div class="card toc">
  <h2>ğŸ“– Table of Contents</h2>
  <ol>
    <li><a href="#chapter1">Chapterâ€¯1 â€“ Introduction</a></li>
    <li><a href="#chapter2">Chapterâ€¯2 â€“ Statistical Learning</a></li>
    <li><a href="#chapter3">Chapterâ€¯3 â€“ Linear Regression</a></li>
    <li><a href="#chapter4">Chapterâ€¯4 â€“ Classification</a></li>
    <li><a href="#chapter5">Chapterâ€¯5 â€“ Resampling Methods</a></li>
    <li><a href="#chapter6">Chapterâ€¯6 â€“ Linear Model Selection</a></li>
    <li><a href="#chapter7">Chapterâ€¯7 â€“ Moving Beyond Linearity</a></li>
    <li><a href="#chapter8">Chapterâ€¯8 â€“ Treeâ€‘Based Methods</a></li>
    <li><a href="#chapter9">Chapterâ€¯9 â€“ Support Vector Machines</a></li>
    <li><a href="#chapter10">Chapterâ€¯10 â€“ Deep Learning</a></li>
  </ol>
</div>

<!-- â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   CHAPTERS   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ -->

<!-- Chapter 1 -->
<div class="card chapter" id="chapter1">
  <h2>ğŸ“— Chapterâ€¯1: Introduction</h2>
  <p>
    The opening chapter motivates <strong>statistical learning</strong> with three flagship datasets:
    <em>Wage</em> (predicting salaries), <em>Smarket</em> (stockâ€‘direction classification), and
    <em>NCI60</em> (geneâ€‘expression clustering).  It distinguishes <strong>supervised</strong> vs
    <strong>unsupervised</strong> learning and clarifies <strong>regression</strong> versus
    <strong>classification</strong> tasks.  Key ideasâ€”<em>predictors</em> &amp; <em>response</em>, <em>training</em> vs
    <em>test error</em>, <strong>overfitting</strong>, and the <strong>biasâ€‘variance tradeâ€‘off</strong>â€”set the stage.  A
    brief history references predecessors like ESL (2001) and ISLR (2013), emphasizing
    Pythonâ€™s growing role.  The chapter also outlines notation (vectorsâ€¯x, matrixâ€¯X,
    responseâ€¯y) and previews the bookâ€™s labs that rely on the <code>ISLP</code> package.
  </p>
  
</div>

<!-- Chapter 2 -->
<div class="card chapter" id="chapter2">
  <h2>ğŸ“˜ Chapterâ€¯2: Statistical Learning</h2>
  <p>
    Formalizes the supervised learning setup \(Y=f(X)+\varepsilon\).
    Contrasts <strong>parametric</strong> models (linear regression) with
    <strong>nonâ€‘parametric</strong> ones (Kâ€‘Nearest Neighbors).  Explores <strong>prediction</strong> vs
    <strong>inference</strong>, introduces <strong>irreducible error</strong>, and quantifies performance via
    <strong>Mean Squared Error (MSE)</strong> and <strong>classification error</strong>.  The
    <em>Advertising</em> dataset illustrates how varying TV, radio, and newspaper budgets
    affect sales.  Concepts like <em>training vs test error</em>, <strong>flexibility</strong>,
    <strong>overfitting</strong>, and the <strong>biasâ€“variance tradeâ€‘off</strong> guide model choice.  The Python
    lab covers NumPy arrays, pandas DataFrames, plotting, and simple KNN fitting.
  </p>
  <p><strong>Lab:</strong> <a href="https://github.com/shyakakevin/Data-Mining-Lab/blob/main/Chapter%202.ipynb" target="_blank">Labâ€¯2 â€“ Statistical Learning</a></p>
</div>

<!-- Chapter 3 -->
<div class="card chapter" id="chapter3">
  <h2>ğŸ“• Chapterâ€¯3: Linear Regression</h2>
  <p>
    Details <strong>simple</strong> and <strong>multiple</strong> linear regression, coefficient estimation by
    <em>least squares</em>, diagnostic tools (RÂ², <em>Residual Standard Error</em>, Fâ€‘statistics),
    and <strong>confidence intervals</strong>.  Demonstrates with the <em>Boston Housing</em> data,
    incorporating quantitative and <strong>qualitative predictors</strong>, plus
    <strong>interaction terms</strong> and <em>transforms</em> like \(X^2\) and \(\log X\).  Addresses
    <strong>multicollinearity</strong>, <em>Variance Inflation Factor (VIF)</em>, and compares linear regression
    against KNN on predictive accuracy.  Python labs use <code>statsmodels</code>,
    design matrices, and residual plots to solidify concepts.
  </p>
  <p><strong>Lab:</strong> <a href="https://github.com/shyakakevin/Data-Mining-Lab/blob/main/Chapter%203.ipynb" target="_blank">Labâ€¯3 â€“ Linear Regression</a></p>
</div>

<!-- Chapter 4 -->
<div class="card chapter" id="chapter4">
  <h2>ğŸ“™ Chapterâ€¯4: Classification</h2>
  <p>
    Presents <strong>Logistic Regression</strong>, <strong>LDA</strong>, <strong>QDA</strong>, <strong>NaiveÂ Bayes</strong>, and
    <strong>KNN</strong>.  Introduces <strong>odds</strong>, <strong>logâ€‘odds</strong>, <em>decision boundaries</em>, and the
    <strong>Bayes Classifier</strong>.  Performance metrics include the
    <em>confusion matrix</em>, <strong>accuracy</strong>, <strong>ROC curve</strong>, <strong>AUC</strong>,
    <em>sensitivity</em>, and <em>specificity</em>.  Using the <em>Smarket</em> data, the text shows why linear
    regression fails for categorical Y and how logistic regression captures
    class probabilities.  Python labs leverage <code>sklearn.metrics</code> and ROC plotting.
  </p>
  <p><strong>Lab:</strong> <a href="https://github.com/shyakakevin/Data-Mining-Lab/blob/main/Chapter%204.ipynb" target="_blank">Labâ€¯4 â€“ Classification</a></p>
</div>

<!-- Chapter 5 -->
<div class="card chapter" id="chapter5">
  <h2>ğŸ“’ Chapterâ€¯5: Resampling Methods</h2>
  <p>
    Explains the <strong>Validationâ€‘Set Approach</strong>, <strong>LOOCV</strong>, and
    <strong>kâ€‘Fold Crossâ€‘Validation</strong> for estimating test error, plus the
    <strong>Bootstrap</strong> for assessing estimator variability.  Connects these ideas to the
    <strong>biasâ€‘variance tradeâ€‘off</strong> and model selection.  Practical examples, including
    the <em>Auto MPG</em> data, show how CV guides polynomial degree choice.  Python
    code employs <code>cross_val_score</code> and custom bootstrap loops.
  </p>
  <p><strong>Lab:</strong> <a href="https://github.com/shyakakevin/Data-Mining-Lab/blob/main/Chatper%205.ipynb" target="_blank">Labâ€¯5 â€“ Resampling</a></p>
</div>

<!-- Chapter 6 -->
<div class="card chapter" id="chapter6">
  <h2>ğŸ“• Chapterâ€¯6: Linear Model Selection &amp; Regularization</h2>
  <p>
    Introduces <strong>bestâ€‘subset</strong> and <strong>stepwise selection</strong>, then moves to
    <strong>shrinkage</strong> via <strong>Ridge Regression</strong> and the <strong>Lasso</strong>, which performs
    <em>automatic variable selection</em>.  Discusses <strong>Principal Components Regression
    (PCR)</strong> and <strong>Partial Least Squares (PLS)</strong> for dimension reduction.  The
    <em>Hitters</em> dataset illustrates how lassoâ€™s \(L_1\) penalty zeros unimportant
    coefficients, improving prediction and interpretability.  Python labs use
    `sklearn.linear_model` and crossâ€‘validation to tune the regularization
    <em>tuning parameter</em> \(\lambda\).
  </p>
  
</div>

<!-- Chapter 7 -->
<div class="card chapter" id="chapter7">
  <h2>ğŸ“— Chapterâ€¯7: Moving Beyond Linearity</h2>
  <p>
    Explores <strong>Polynomial Regression</strong>, <strong>Step Functions</strong>, <strong>Regression Splines</strong>,
    <strong>Smoothing Splines</strong>, <strong>Local Regression</strong>, and
    <strong>Generalized Additive Models (GAMs)</strong>.  Concepts of <em>knots</em>, <em>basis
    functions</em>, and <em>degrees of freedom</em> control flexibility.  The <em>Wage</em> data shows
    how cubic splines capture the nonâ€‘linear wageâ€‘age relationship better than
    a straight line.  GAMs provide additive smooth terms plus interpretability.
    Implementation uses `patsy` splines and <code>pygam</code> for automatic smoothing.
  </p>
  
</div>

<!-- Chapter 8 -->
<div class="card chapter" id="chapter8">
  <h2>ğŸ“˜ Chapterâ€¯8: Treeâ€‘Based Methods</h2>
  <p>
    Starts with <strong>Decision Trees</strong> for regression and classification, explaining
    <em>recursive binary splitting</em>, <strong>costâ€‘complexity pruning</strong>, and metrics like the
    <strong>Gini index</strong>.  Ensemble methods follow: <strong>Bagging</strong> (bootstrap
    aggregation), <strong>RandomÂ Forests</strong> (decorrelated trees via random feature
    selection), and <strong>Boosting</strong> (sequentially improving <em>weak learners</em>).  The
    <em>Carseats</em> data exemplifies how boosting lowers test error.  Labs rely on
    `sklearn.tree`, `RandomForestClassifier`, and `GradientBoostingRegressor`.
  </p>
  <p><strong>Lab:</strong> <a href="https://github.com/shyakakevin/Data-Mining-Lab/blob/main/Chapter%208.ipynb" target="_blank">Labâ€¯8 â€“ Treeâ€‘Based Methods</a></p>
</div>

<!-- Chapter 9 -->
<div class="card chapter" id="chapter9">
  <h2>ğŸ“™ Chapterâ€¯9: Support Vector Machines</h2>
  <p>
    Builds from the <strong>Maximal Margin Classifier</strong> to <strong>Support Vector
    Classifiers</strong> by allowing <em>soft margins</em>.  Introduces the <strong>kernel trick</strong> to create
    nonâ€‘linear boundaries in highâ€‘dimensional <em>feature space</em>, using
    <strong>polynomial</strong> and <strong>radial basis</strong> kernels.  Key terms include <em>support
    vectors</em>, <em>slack variables</em>, and the <em>tuning parameter</em> \(C\).  The <em>HeartÂ Disease</em>
    dataset demonstrates ROC optimization and multiâ€‘class extensions
    (â€œoneâ€‘vsâ€‘oneâ€, â€œoneâ€‘vsâ€‘allâ€) via scikitâ€‘learnâ€™s <code>SVC</code>.
  </p>
  <p><strong>Lab:</strong> <a href="https://github.com/shyakakevin/Data-Mining-Lab/blob/main/Chapter%209.ipynb">LabÂ 9 â€“ SVM (coming soon)</a></p>
</div>

<!-- Chapter 10 -->
<div class="card chapter" id="chapter10">
  <h2>ğŸ“• Chapterâ€¯10: Deep Learning</h2>
  <p>
    Covers <strong>feedâ€‘forward neural networks</strong>, <strong>backpropagation</strong>,
    <strong>activation functions</strong> (ReLU, sigmoid), <strong>regularization</strong> (dropout,
    weight decay), and optimization (SGD, Adam).  Specialized architecturesâ€”
    <strong>Convolutional Neural Networks (CNNs)</strong> for images and
    <strong>Recurrent Neural Networks (RNNs)</strong> for sequencesâ€”are introduced.  The
    chapter explains <strong>overâ€‘parameterization</strong> and <em>doubleâ€‘descent</em>.  Handsâ€‘on labs
    classify digits with <em>MNIST</em> and sentiment with <em>IMDB</em>, using <code>tensorflow</code> /
    <code>keras</code>.
  </p>
  
</div>

<!-- â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   ALGORITHMS   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ -->
<div class="card">
  <h2>ğŸ§  Algorithms by Task</h2>

  <h3>ğŸ”µ Regression</h3>
  <ul>
    <li>Simple / Multiple Linear Regression</li>
    <li>Polynomial Regression &amp; Splines</li>
    <li>Ridge Regression &amp; Lasso</li>
    <li>Generalized Additive Models (GAMs)</li>
    <li>Regression Trees &amp; Random Forests</li>
  </ul>

  <h3>ğŸŸ¢ Classification</h3>
  <ul>
    <li>Logistic Regression</li>
    <li>LDA / QDA</li>
    <li>Kâ€‘Nearest Neighbors (KNN)</li>
    <li>Classification Trees &amp; Boosting</li>
    <li>Support Vector Machines (SVM)</li>
    <li>Neural Networks &amp; CNNs</li>
  </ul>

  <h3>ğŸŸ£ Unsupervised Learning</h3>
  <ul>
    <li>Principal Component Analysis (PCA)</li>
    <li>Kâ€‘Means &amp; Hierarchical Clustering</li>
    <li>Autoencoder Embeddings</li>
  </ul>
</div>

<!-- â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   FOOTER   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ -->
<footer>
  PreparedÂ byÂ ShyakaÂ Kevin â€“ MScÂ BigÂ DataÂ Analytics â€“ JulyÂ 2025
</footer>

</body>
</html>